{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 第2章 ファインチューニング: 言語モデルの追加学習",
   "id": "16afe85460dabcc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.573877Z",
     "start_time": "2024-07-21T20:49:02.760403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"cyberagent/open-calm-small\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ],
   "id": "81738d733dca0ab6",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.577555Z",
     "start_time": "2024-07-21T20:49:04.573877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = tokenizer.encode(\"私は犬が好き。\", return_tensors=\"pt\")\n",
    "print(input)"
   ],
   "id": "c84a71634c3ab6fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2727, 3807, 9439,  247]])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.588724Z",
     "start_time": "2024-07-21T20:49:04.577555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = [tokenizer.decode(input[0][i]) for i in range(len(input[0]))]\n",
    "print(a)"
   ],
   "id": "2f321e9a0c2432a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私は', '犬', 'が好き', '。']\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.621624Z",
     "start_time": "2024-07-21T20:49:04.588724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(input)\n",
    "print(type(output))\n",
    "print(output.logits)\n",
    "print(output.logits.shape)"
   ],
   "id": "82467d962f6f037c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "tensor([[[  7.2745, -17.2270,   8.0928,  ..., -16.3880, -17.2634, -17.3245],\n",
      "         [  7.2613, -16.8069,  10.7188,  ..., -16.0717, -17.0571, -16.9093],\n",
      "         [ 12.8202, -16.5409,  15.7865,  ..., -15.9594, -16.8466, -16.8622],\n",
      "         [ 12.0014, -17.0628,   7.1439,  ..., -16.2650, -17.1278, -17.2398]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 4, 52096])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.628461Z",
     "start_time": "2024-07-21T20:49:04.622627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 各tokenと入力文に対する損失値の出力\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "from_tensor = torch.cat([input[0][1:], torch.tensor([-100])])\n",
    "print(from_tensor)\n",
    "loss0 = loss_fn(output.logits[0], from_tensor)\n",
    "print(loss0)\n",
    "# tokenの損失値の平均\n",
    "print(torch.sum(loss0) / 3)"
   ],
   "id": "80068d80337ea867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3807, 9439,  247, -100])\n",
      "tensor([8.0847, 5.4859, 3.0050, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5252, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.642407Z",
     "start_time": "2024-07-21T20:49:04.629464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 入力分に対する損失値の出力\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss1 = loss_fn(output.logits[0], from_tensor)\n",
    "print(loss1)"
   ],
   "id": "cbe77a7d5093fe96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5252, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.673719Z",
     "start_time": "2024-07-21T20:49:04.642407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# `model` を利用して自動で損失値を計算\n",
    "output = model(input, labels=input)\n",
    "loss = output.loss\n",
    "print(loss)"
   ],
   "id": "6361a92f4459701c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5252, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:04.801074Z",
     "start_time": "2024-07-21T20:49:04.674722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# パラメータの更新\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "id": "32203e43761ce032",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "言語モデルの追加学習は、コーパスの各文に対して、パラメータの更新処理を繰り返して実行する。",
   "id": "ad174f1b1b05dbe9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.2 Trainerの利用",
   "id": "455608b34b274e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:05.240672Z",
     "start_time": "2024-07-21T20:49:04.802078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=5,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    ")"
   ],
   "id": "db3687246fa12091",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## データ利用元\n",
    "\n",
    "https://sites.google.com/view/kvjcorpus/%E3%83%9B%E3%83%BC%E3%83%A0/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB?authuser=0\n",
    "\n",
    "ヘファナン・ケビン（2012）「関西弁コーパスの紹介」『総合政策研究』41号 157-164.\n",
    "\n",
    "- ./chap2-kjs-corpus/test.txt\n",
    "- ./chap2-kjs-corpus/train.txt\n",
    "- ./chap2-kjs-corpus/val.txt"
   ],
   "id": "c50275eb0c5f500f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:09.470734Z",
     "start_time": "2024-07-21T20:49:05.241677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, filename, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.features = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            lines = f.read().split(\"\\n\")\n",
    "            for line in lines:\n",
    "                input_ids = tokenizer.encode(line, return_tensors=\"pt\", max_length=512, padding=\"longest\")[0]\n",
    "                self.features.append({\"input_ids\": input_ids})\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "train_dataset = MyDataset(\"./chap2-kjs-corpus/train.txt\", tokenizer)"
   ],
   "id": "84af5eda7b398af8",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:09.474455Z",
     "start_time": "2024-07-21T20:49:09.471740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ],
   "id": "dbcf59b9db4de046",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:09.484648Z",
     "start_time": "2024-07-21T20:49:09.474633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collator)"
   ],
   "id": "21ed3e145e819ad4",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:09.555617Z",
     "start_time": "2024-07-21T20:49:09.484648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dl = iter(dataloader)\n",
    "a = next(dl)\n",
    "print(a)\n",
    "# paddingが行われていることが確認できる"
   ],
   "id": "fb443f9c29d45dbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 3479,  5955,  4180,   245,  5020,  1332,  2443, 21892,   258,  2076,\n",
      "           338,   529,   256,   245,  8267,  2831,   245,   623,  5081,   267,\n",
      "           256,  3467,   247],\n",
      "        [  592, 21164,   270,  2868,  7253, 12127, 34609,   247,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [18092, 12172, 30105, 28820,    32,   279, 46776,   245,   676,   247,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [  308, 25821, 42586,   247,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [51901,    32, 51901,  3972, 23848,   463, 27916,   314,   259,   247,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [ 5020, 14190,   363,   245,  4886,   416,   480, 17028, 36672,   245,\n",
      "          1733,  1119,   247,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [ 5645,  4517, 15416,  3136,  2560,   434,   363,  2696,  2889,   250,\n",
      "            32, 35347,   245,   316, 14667,   584, 46964,   247,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [  399,   343, 12643,   245,  5938, 11724,  4675, 30102,  3313, 21313,\n",
      "           374,   259,   247,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [ 8665,   245,  4023,   581,  8013, 22686,  5074,   257, 45827,   259,\n",
      "           247,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1],\n",
      "        [  689,   247,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[ 3479,  5955,  4180,   245,  5020,  1332,  2443, 21892,   258,  2076,\n",
      "           338,   529,   256,   245,  8267,  2831,   245,   623,  5081,   267,\n",
      "           256,  3467,   247],\n",
      "        [  592, 21164,   270,  2868,  7253, 12127, 34609,   247,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [18092, 12172, 30105, 28820,    32,   279, 46776,   245,   676,   247,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [  308, 25821, 42586,   247,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [51901,    32, 51901,  3972, 23848,   463, 27916,   314,   259,   247,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [ 5020, 14190,   363,   245,  4886,   416,   480, 17028, 36672,   245,\n",
      "          1733,  1119,   247,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [ 5645,  4517, 15416,  3136,  2560,   434,   363,  2696,  2889,   250,\n",
      "            32, 35347,   245,   316, 14667,   584, 46964,   247,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [  399,   343, 12643,   245,  5938, 11724,  4675, 30102,  3313, 21313,\n",
      "           374,   259,   247,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [ 8665,   245,  4023,   581,  8013, 22686,  5074,   257, 45827,   259,\n",
      "           247,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100],\n",
      "        [  689,   247,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.5 保存されたモデルからの文生成",
   "id": "53985fd77117e578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T20:49:09.557927Z",
     "start_time": "2024-07-21T20:49:09.555617Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "27a2a826d5cbd014",
   "outputs": [],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
